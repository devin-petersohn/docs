%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}

\usepackage{url}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{wrapfig} % added from executive summary
\usepackage{etaremune}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{listings} 

\theoremstyle{definition}
\newtheorem{defn}{Definition}

\lstdefinelanguage{scala}{morekeywords={class,object,trait,extends,with,new,if,while,for,def,val,var,this},
otherkeywords={->,=>},
sensitive=true,
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{PPoPP '15}{February 7--11, 2015, San Francisco, CA, USA} 
\copyrightyear{2015}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Partition Balancing in Distributed Nested Data-Parallel Systems}
\preprintfooter{DRAFT---do not distribute} 


\title{Partition Balancing in Distributed Nested Data-Parallel Systems}
\authorinfo{Frank~Austin~Nothaft}
{Department of Electrical Engineering and Computer Science, University of California, Berkeley}
{fnothaft@eecs.berkeley.edu}

\authorinfo{Michael Linderman}
{Carl Icahn School of Medicine at Mount Sinai}
{michael.linderman@mssm.edu}

\maketitle

\raggedbottom

\begin{abstract}

Map-reduce frameworks such as Apache Hadoop and Spark provide the abstraction of a large, flat
array that is processed in parallel across many machines. While this simple programming model has
enabled the broad adoption of data-parallel distributed frameworks, these systems cannot express
irregular parallel computation, and their performance is impacted by data imbalance across nodes.
In this paper, we present a distributed framework for implementing \emph{nested data parallel}~(NDP)
computation. Unlike previous NDP systems described by Blelloch and Bergstrom et al. that relied on
the use of a \emph{flattening} transformation at compile-time, we present a cost model that is used
to select between multiple partitioning strategies at run-time. Through this, we provide a user-tunable
means for trading node-to-node imbalance versus communication when executing distributed NDP
programs.

\end{abstract}

\section{Introduction}
\label{sec:introduction}

The use of map-reduce as a flat data-parallel~(FDP) programming model for distributed systems has
grown rapidly since it was introduced in Google's seminal 2004 paper~\cite{dean04}. The development
of the open-source Apache Hadoop system enabled the use of this programming model outside of
Google. Modern map-reduce systems such as Apache Spark~\cite{zaharia10} have refined the
programming model further by reducing the dependency of the framework on disk via in-memory
caching. While this refinement has enabled the use of map-reduce for iterative workloads, the
programming model remains confined to computation that can be expressed via flat data-parallel
operations. Specifically, Spark presents users with the abstraction of a resilient distributed
dataset~(RDD), which appears as a flat array that is distributed across compute nodes in a
cluster~\cite{zaharia12}.

To expand the algorithms that could be expressed as a data-parallel computation, Blelloch introduced
the nested data-\linebreak parallel~(NDP) model~\cite{blelloch90thesis}. In this programming model,
users are provided the abstraction of an array whose elements are a nested level of arrays and
data-parallel operators are applied on the nested arrays. A primary complication in the implementation
of the NDP model is that the nested arrays frequently do not have uniform size. Several approaches
have been suggested for balancing work in NDP programs, including the compile-time vectorization of
NDP programs for execution on single-instruction multiple-data~(SIMD)
machines~\cite{blelloch90thesis, blelloch90compiling} and flattening for multiple-instruction
multiple-data~(MIMD) machines~\cite{bergstrom13}. Additionally, dynamic work-stealing approaches
have been implemented~\cite{bergstrom10}.

In this paper, we introduce a distributed programming framework for NDP algorithms. Our
implementation is built on top of Apache Spark. We track the structure of the nested arrays at runtime
and choose between two strategies~(\emph{uniform} and \emph{segmented}) for partitioning data
across machines based on the estimated cost of each strategy. In the segmented partitioning strategy,
all values in a single nested segment are co-located on a single compute node, and most computation
can proceed without communication. The uniform strategy provides perfect load balance across all
nodes, but many operations will need to communicate to execute. To choose between these strategies
at runtime, we provide a cost model that evaluates the performance tradeoff of communication overhead 
versus node-to-node imbalance. Since the partitioning strategy is chosen at runtime, we can leverage
knowledge of the nested array structure.

This work introduces the following contributions:

\begin{enumerate}
\item We introduce a dynamically optimized distributed method for implementing NDP algorithms that is
amenable to ``cloud computing'' platforms.
\item We demonstrate that the current best-known \texttt{scan} algorithm has $O(\frac{n \log p}{p})$
performance on bulk-synchronous systems, and provide a ``block-parallel'' relaxation of the \texttt{scan}
operator that has $O(\frac{n}{p})$ performance.
\item We demonstrate a model-based approach for balancing partition load in distributed systems.
\end{enumerate}

\section{Background}
\label{sec:background}

Understanding the system we describe in the rest of the paper requires an understanding of the NDP
processing model, as well as the design of current MapReduce-style distributed computing frameworks.
In this section, we compare and contrast the FDP and NDP programming models, and discuss the
system design of the Spark MapReduce framework. While the problem of implementing scalable
distributed FDP frameworks is well studied~\cite{dean04, zaharia10}, this paper builds upon those
systems to focus on the implementation of scalable distributed NDP platforms.

\subsection{Data-Parallel Programming Models}
\label{sec:data-parallel-programming-models}

In data-parallel frameworks, processing proceeds in parallel across all of the elements in a dataset. We
can further subdivide data-parallel programming models into flat and nested data parallelism:

\paragraph{Flat data-parallelism:}

In a FDP system, the parallel collection contains single elements which are then processed in parallel.
On a collection containing elements of type $T$, a few basic operations include:

\begin{itemize}
\item \texttt{map}, which applies a function $F: T \mapsto U$ to all elements of the collection, and
returns a new collection containing elements of type $U$
\item \texttt{reduce}, which successively applies a function $F: T, T \mapsto T$ to pairs of elements from
the collection until all elements have been reduced down into a single scalar of type $T$
\end{itemize}

FDP may also be known as \emph{regular} data parallelism, as the data parallel computation proceeds
over the single items in the dataset~(all parallel computation occurs on single elements, i.e.,
``collections'' of size 1). Because there are no dependencies between elements in the dataset, FDP can
implemented efficiently on a broad range of computing platforms, including SIMD and MIMD
architectures, graphics processing units~(GPUs)~\cite{nvidia08}, and distributed systems~\cite{dean04}.

\paragraph{Nested data-parallelism:}

An NDP system extends the collection structure of an FDP system. Instead of a flat collection, the
collection is divided into \emph{segments} or \emph{nests}, which are sub-groupings of elements.
Operations may then execute across the whole collection, or across the segments inside of the
collection. A full introduction to NDP can be found in the text by Blelloch~\cite{blelloch90thesis}. Blelloch
justifies the utility of the NDP model by noting that NDP reduces the asymptotic runtime of some graph
and linear algebra algorithms by a factor of $\log n$. A good demonstration of the utility of the nested
vector model comes from implementing a matrix-vector multiply. Here, we assume that we have a matrix
$A$ which has been packed into a nested vector by splitting each row $i$ into it's own segment, and a
dense vector $v$. We can implement the matrix-vector multiply by:

\begin{lstlisting}
def matrix-vector-multiply(A: SegmentedArray[Int],
					v: Array[Int]
  ): Array[Int] = {
  A.map-with-index((point: Int, idx: Int) => {
    point * v(idx)
  }).segmented-reduce(_ + _)
}
\end{lstlisting}

Here, we use the \texttt{map-with-index} function to multiply each point in the matrix by the
corresponding point in the vector. This can execute in parallel per element. The
\texttt{segmented-reduce} function then sums up all of the elements from each row. The na\"{i}ve
implementation of a \texttt{segmented-reduce} is parallel per segment, but an optimized implementation
is parallel per element in the whole collection.

\subsection{Data-Parallel Distributed Computing}
\label{sec:data-parallel-distributed-computing}

This work builds upon the infrastructure provided by the Apache Spark distributed data-parallel
computing framework~\cite{spark, zaharia10}. Spark was designed for ``cloud computing'' platforms,
where machines may be unreliable, and where network performance may preclude the use of traditional
distributed message passing systems such as the Message Passing Interface~(MPI). Unlike Apache
Hadoop, where data is shuffled to/from disk between all processing stages~\cite{hadoop, shvachko10},
Spark has an in-memory processing model. The in-memory processing model leads to
large~($100\times$) performance for iterative jobs implemented using Spark instead of
Hadoop~\cite{zaharia10}.

Spark's programming model is implemented on top of the \emph{resilient distributed dataset}~(RDD)
abstraction~\cite{zaharia12}. The RDD abstraction presents a view of an array which is chopped into
\emph{partitions} that are distributed over the computers within the Spark cluster. Programs enqueue
data-parallel transformations on the Spark master, which are then interpreted into a directed-acyclic
graph~(DAG) for execution; this approach is similar to the DAG scheduling approach pioneered in
Dryad~\cite{isard07}. Spark executes the DAG whenever a disk shuffle is required, or if a newly
enqueued operation would create a cycle in the DAG~(i.e., an iterative computation is scheduled).

To execute a stage, the Spark master serializes the user defined function~(UDF) which is being applied,
transmits this function to all worker nodes, and then applies the computation. While the general
application programming interface~(API) that Spark provides is data-parallel across all elements,
the API transformations are implemented by applying the transformation iteratively across all elements
of a partition---Spark also exposes this parallel-by-partition interface via the \texttt{mapPartitions} call.
If data must be moved between partitions after an execution stage, a disk shuffle will occur. Disk
shuffles are analogous to an execution stage with interleaved computation and all-to-all communication
followed by a barrier before the next stage. This process is similar to how the results of \texttt{map}
phases are moved to reducers in both MapReduce and Hadoop~\cite{dean04}---the main distinction is
that Spark allows successive \texttt{map} phases, and that shuffles do not occur after all stages.

\section{Implementation}
\label{sec:implementation}

\subsection{Characterizing and Modeling Imbalance}
\label{sec:imbalance}

\subsection{Partitioning Strategies}
\label{sec:partitioning-strategies}

Given the performance model introduced in~\S\ref{sec:imbalance}, we introduce two separate strategies
for data partitioning:

\begin{itemize}
\item \texttt{segmented}: In this strategy, each segment/nest of the nested array is allocated a single
partition of the RDD. Unless the size of each segment is identical, the size of each partition may vary.
\item \texttt{uniform}: In this strategy, each partition of the RDD is allocated the same number of
values; therefore, the partition sizes are uniform. In this strategy, a single segment may be split across
one or more partitions.
\end{itemize}

If the number of segments equals the number of partitions, and all segments have identical length,
both strategies are identical and reduce to the \texttt{segmented} strategy.

We face the following tradeoffs when picking a strategy for partitioning a dataset:

\begin{itemize}
\item \emph{How well will load be balanced?} The \texttt{uniform} strategy is highly desirable
because we are able to perfectly allocate the keys to partitions, and load is balanced. However, in
practice, load balance can be improved by oversubscribing partitions to processing elements~(see
\emph{Tuning Guide} in~\cite{spark}; typically, an oversubscription factor of 3 is used). If there is
oversubscription, we may be able to co-schedule small and large partitions on the same processing
element. While we may have imbalance between each partition, we may be able to achieve balance
between processing elements.
\item \emph{How much overhead does the strategy incur?} While the \linebreak \texttt{segmented}
strategy may have increased imbalance, it has low overhead because we can guarantee that all
segment-based operations complete without requiring communication between partitions. This is
desirable because the block synchronous model common to MapReduce frameworks implies that all
communication requires a barrier.
\end{itemize}

The additional overhead of the \texttt{uniform} partitioning model is limited to the
\texttt{segmented-scan} operation. To implement a \linebreak\texttt{segmented-scan}, we fall back on
the block-parallel \texttt{scan} implementation that we introduce in~\S\ref{sec:block-parallel-scans}.
There are several additional complications due to the segmented nature of the \texttt{scan}; specifically,
we must track whether the results generated by previous partitions are related to this segment. This
requires us to track additional information during the \emph{pre-scan} phase, and necessitates filtering
when performing the \emph{update} phase.

While it may appear that the \texttt{segmented-reduce} operation will also incur additional overhead, this
overhead is negligible. For the \texttt{segmented} partitioning strategy, each partition performs a
reduction across its elements, and then sends the results to the master. The \texttt{uniform} strategy
performs a keyed reduction---we only reduce together values that are from the same segment. Thus, if a
partition contains values from $s$ segments, it will compute $s$ reduced values (one per segment) after
the initial partition reduction. We label these values with the segment number, collect the values to
the driver, and reduce together values with like labels. If we ensure that the reduction operator is
associative and commutative, this approach will be correct. Overhead is limited, as generally $n_p \gg
s_p$, where $n_p$ is the number of values in partition $p$, and $s_p$ is the number of segments with
values in partition $p$.

The structured nature of a nested array provides a significant advantage over unstructured data-parallel
models, as it enforces a rigid, known, and efficiently quantifiable order across the data. For unordered
datasets, a hash function must be used to partition data in parallel across nodes~\cite{dewitt90,
dean04}. Even for ordered datasets, finding an even partitioning is expensive; in Spark, sorting
requires the calculation of histogram describing the distribution of keys across the ordering~\cite{spark}.
Since the key distribution is described by the array structure, and a nested array structure can be
described by an array of lengths, we can statically track the whole ordering on the master node, instead
of needing to recalculate the key distribution before repartitioning.

\subsection{Block-Parallel Scans}
\label{sec:block-parallel-scans}

The \texttt{scan}\footnote{The \texttt{scan} operator is also known as the \emph{all-prefix
sum}~\cite{blelloch93}.} operator is challenging to implement in parallel, as the scan operation on array
element $a_n$ depends on the values of all elements $a_0 \ldots a_{n - 1}$. For clarity, we
reproduce the definition of a \texttt{scan} given by Blelloch~\cite{blelloch93} here:

\begin{defn}
\label{defn:scan}
The \texttt{scan} operator takes an associative operator $\oplus$, and an ordered set of $n$ elements:
$$
[a_0, a_1, \ldots, a_{n - 1}]
$$
and returns the ordered set:
$$
[a_0, (a_0 \oplus a_1), \ldots, (a_0 \oplus a_1 \oplus \ldots \oplus a_{n - 1})]
$$
\end{defn}

Blelloch proposed a parallel algorithm for computing vector scans~\cite{blelloch93}; this algorithm has
recently been implemented for GPUs~\cite{harris07}. This algorithm is composed of two basic steps:

\begin{enumerate}
\item First, perform an ``up sweep'' by performing a parallel tree-reduction across the vector.
\item ``Down sweep'' by shifting in the identity value and rotating the tree elements.
\end{enumerate}

A detailed description of this algorithm, as well as a proof of correctness is presented by both
Blelloch~\cite{blelloch93} and Harris~\cite{harris07}. This algorithm completes in $O(\frac{n}{p} +
\log p)$ time when evaluating a vector of $n$ elements on $p$ processors. For $n \gg p$, the
runtime is approximated as $O(\frac{n}{p})$. Despite the efficient runtime, the \emph{up sweep,
down sweep} algorithm has several drawbacks:

\begin{itemize}
\item For a vector of type $T$, the $\oplus$ operator is restricted to $T, T \mapsto T$. This is an
unnecessary restriction if we loosen definition~\ref{defn:scan} to include an additional input variable
$b_0$, which is the additive identity value. If $b_0$ has type $U$, this allows $\oplus$ to become $U, T
\mapsto U$. We include this in our updated definition~\ref{defn:block-parallel-scan}.
\item The \emph{up sweep, down sweep} algorithm only has $O(\frac{n}{p})$ runtime when executing
on systems with a parallel random access memory~(PRAM) compute model. As described
in~\S\ref{sec:data-parallel-distributed-computing}, MapReduce computing systems provide a block
synchronous model. For this model, the runtime degrades to $O(\frac{n \log p}{p})$, as we have
$\frac{n}{p}$ runtime for each of the $\log p$ stages in both the \emph{up sweep} and \emph{down
sweep} stages.
\end{itemize}

To address the issues we have just presented, we introduce a block-parallel scan:

\begin{defn}
\label{defn:block-parallel-scan}
The block-parallel \texttt{scan} operator takes an associative \emph{scan} operator $\oplus$ ($U, T
\mapsto U$), and an associative \emph{update} operator $\otimes$ ($U, U \mapsto U$), an identity
value $b_0$, and an ordered set of $n$ elements:

$$
A = [a_0, a_1, \ldots, a_{n - 1}]
$$

The ordered set A is partitioned into $p$ ordered partitions:

\begin{equation*}
\begin{split}
P^0 &= [a_0, a_1, \ldots, a_{\frac{n}{p} - 1}] \\
P^1 &= [a_{\frac{n}{p}}, a_{\frac{n}{p} + 1}, \ldots, a_{2\frac{n}{p} - 1}] \\
&\ldots \\
P^{p - 1} &= [a_{(p - 1)\frac{n}{p}}, a_{(p - 1)\frac{n}{p} + 1}, \ldots, a_{n - 1}] \\
A &= P^0 \cup P^1 \cup \ldots \cup P^{p - 1}
\end{split}
\end{equation*}

For notational convenience, we assume that $n \bmod p = 0$.

For all $P^k, k \in \{0, \ldots, p - 1\}$, we compute intermediate \emph{block pre-scans} $S^k$ where:

\begin{equation*}
\begin{split}
S^k_0 &= b_0 \\
S^k_i &= S^k_{i - 1} \oplus P^k_{i - 1}, \forall i \in {1, \ldots, p}
\end{split}
\end{equation*}

Note that all sets $S_k$ have size $p + 1$, not $p$.

For all $S^k, k \in \{0, \ldots, p - 1\}$, we then collect the $S^k_p$ terms into set $C$, where $C_k =
S^k_p$. We then perform the \emph{update} phase across all $S^k$:

\begin{equation*}
\begin{split}
u^k &= \begin{cases}
    b_0& \text{if } k = 0\\
    C_0 \otimes \ldots \otimes C_{k - 1}, & \text{otherwise}
\end{cases} \\
U^k_i &= u_k \otimes S^k_i, \forall i \in \{1, \ldots, p\}
\end{split}
\end{equation*}

We return the ordered set:
\begin{equation*}
\begin{split}
S &=U_0 \cup U_1 \cup \ldots \cup U_{p - 1} \\
S &= [b_0 \oplus a_0, \ldots, b_0 \oplus a_0 \oplus \ldots \oplus a_{\frac{n}{p} - 1}, \ldots, \\
 &(b_0 \oplus
a_0 \oplus \ldots \oplus a_{\frac{n}{p} - 1}) \otimes (b_0 \oplus a_{\frac{n}{p}} \oplus \ldots \oplus
a_{2\frac{n}{p} - 1}) \otimes \ldots \\
& \otimes (b_0 \oplus a_{(p - 1)\frac{n}{p}} \oplus \ldots \oplus a_{n - 1})]
\end{split}
\end{equation*}
\end{defn}

It is not possible to generally prove that the \texttt{scan} operator is equal to the block-parallel
\texttt{scan} operator for all general $\oplus$ operators. However, it is straightforward to prove
equivalence for some common operators. For example, given $T = U =$ integer, and $\oplus = +
= \otimes$, it follows that $b_0 = 0$. In this case, all of the $b_0$ terms drop out, and $\oplus =
\otimes$, therefore the block-parallel \texttt{scan} operation matches the \texttt{scan} operation.

Both the per-block \emph{pre-scan} and the \emph{update} operations from
definition~\ref{defn:block-parallel-scan} have runtime $O(\frac{n}{p})$. On the computing platforms we
are targeting, the cost of collecting a small amount of data from each partition is negligible. Therefore,
the block-parallel \texttt{scan} has runtime $O(\frac{n}{p})$. We are able to use the block-parallel
\texttt{scan} algorithm to perform efficient parallel \texttt{scan}s across all datasets, and to accelerate
segmented scans when using the \texttt{uniform} partitioning strategy introduced
in~\S\ref{sec:partitioning-strategies}.

\section{Performance}
\label{sec:performance}

\section{Discussion}
\label{sec:discussion}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{acm}

\bibliography{snark}

\end{document}