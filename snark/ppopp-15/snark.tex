%-----------------------------------------------------------------------------
%
%               Template for sigplanconf LaTeX Class
%
% Name:         sigplanconf-template.tex
%
% Purpose:      A template for sigplanconf.cls, which is a LaTeX 2e class
%               file for SIGPLAN conference proceedings.
%
% Guide:        Refer to "Author's Guide to the ACM SIGPLAN Class,"
%               sigplanconf-guide.pdf
%
% Author:       Paul C. Anagnostopoulos
%               Windfall Software
%               978 371-2316
%               paul@windfall.com
%
% Created:      15 February 2005
%
%-----------------------------------------------------------------------------


\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}

\usepackage{url}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{wrapfig} % added from executive summary
\usepackage{etaremune}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{listings} 

\theoremstyle{plain}


\lstdefinelanguage{scala}{morekeywords={class,object,trait,extends,with,new,if,while,for,def,val,var,this},
otherkeywords={->,=>},
sensitive=true,
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily}}

\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{PPoPP '15}{February 7--11, 2015, San Francisco, CA, USA} 
\copyrightyear{2015}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}

% Uncomment one of the following two, if you are not going for the 
% traditional copyright transfer agreement.

%\exclusivelicense                % ACM gets exclusive license to publish, 
                                  % you retain copyright

%\permissiontopublish             % ACM gets nonexclusive license to publish
                                  % (paid open-access papers, 
                                  % short abstracts)

\titlebanner{Partition Balancing in Distributed Nested Data-Parallel Systems}
\preprintfooter{DRAFT---do not distribute} 


\title{Partition Balancing in Distributed Nested Data-Parallel Systems}
\authorinfo{Frank~Austin~Nothaft}
{Department of Electrical Engineering and Computer Science, University of California, Berkeley}
{fnothaft@eecs.berkeley.edu}

\authorinfo{Michael Linderman}
{Carl Icahn School of Medicine at Mount Sinai}
{michael.linderman@mssm.edu}

\maketitle

\raggedbottom

\begin{abstract}

Map-reduce frameworks such as Apache Hadoop and Spark provide the abstraction of a large, flat
array that is processed in parallel across many machines. While this simple programming model has
enabled the broad adoption of data-parallel distributed frameworks, these systems cannot express
irregular parallel computation, and their performance is impacted by data imbalance across nodes.
In this paper, we present a distributed framework for implementing \emph{nested data parallel}~(NDP)
computation. Unlike previous NDP systems described by Blelloch and Bergstrom et al. that relied on
the use of a \emph{flattening} transformation at compile-time, we present a cost model that is used
to select between multiple partitioning strategies at run-time. Through this, we provide a user-tunable
means for trading node-to-node imbalance versus communication when executing distributed NDP
programs.

\end{abstract}

\section{Introduction}
\label{sec:introduction}

The use of map-reduce as a flat data-parallel~(FDP) programming model for distributed systems has
grown rapidly since it was introduced in Google's seminal 2004 paper~\cite{dean04}. The development
of the open-source Apache Hadoop system enabled the use of this programming model outside of
Google. Modern map-reduce systems such as Apache Spark~\cite{zaharia10} have refined the
programming model further by reducing the dependency of the framework on disk via in-memory
caching. While this refinement has enabled the use of map-reduce for iterative workloads, the
programming model remains confined to computation that can be expressed via flat data-parallel
operations. Specifically, Spark presents users with the abstraction of a resilient distributed
dataset~(RDD), which appears as a flat array that is distributed across compute nodes in a
cluster~\cite{zaharia12}.

To expand the algorithms that could be expressed as a data-parallel computation, Blelloch introduced
the nested data-\linebreak parallel~(NDP) model~\cite{blelloch90thesis}. In this programming model,
users are provided the abstraction of an array whose elements are a nested level of arrays and
data-parallel operators are applied on the nested arrays. A primary complication in the implementation
of the NDP model is that the nested arrays frequently do not have uniform size. Several approaches
have been suggested for balancing work in NDP programs, including the compile-time vectorization of
NDP programs for execution on single-instruction multiple-data~(SIMD)
machines~\cite{blelloch90thesis, blelloch90compiling} and flattening for multiple-instruction
multiple-data~(MIMD) machines~\cite{bergstrom13}. Additionally, dynamic work-stealing approaches
have been implemented~\cite{bergstrom10}.

In this paper, we introduce a distributed programming framework for NDP algorithms. Our
implementation is built on top of Apache Spark. We track the structure of the nested arrays at runtime
and choose between two strategies~(\emph{uniform} and \emph{segmented}) for partitioning data
across machines based on the estimated cost of each strategy. In the segmented partitioning strategy,
all values in a single nested segment are co-located on a single compute node, and most computation
can proceed without communication. The uniform strategy provides perfect load balance across all
nodes, but many operations will need to communicate to execute. To choose between these strategies
at runtime, we provide a cost model that evaluates the performance tradeoff of communication overhead 
versus node-to-node imbalance. Since the partitioning strategy is chosen at runtime, we can leverage
knowledge of the nested array structure.

\section{Background}
\label{sec:background}

\subsection{Data-Parallel Programming Models}
\label{sec:data-parallel-programming-models}



\subsection{Data-Parallel Distributed Computing}
\label{sec:data-parallel-distributed-computing}

This work builds upon the infrastructure provided by the Apache Spark distributed data-parallel
computing framework~\cite{spark, zaharia10}. Spark was designed for ``cloud computing'' platforms,
where machines may be unreliable, and where network performance may preclude the use of traditional
distributed message passing systems such as the Message Passing Interface~(MPI). Unlike Apache
Hadoop, where data is shuffled to/from disk between all processing stages~\cite{hadoop, shvachko10},
Spark has an in-memory processing model. The in-memory processing model leads to
large~($100\times$) performance for iterative jobs implemented using Spark instead of
Hadoop~\cite{zaharia10}.

Spark's programming model is implemented on top of the \emph{resilient distributed dataset}~(RDD)
abstraction~\cite{zaharia12}. The RDD abstraction presents a view of an array which is chopped into
\emph{partitions} that are distributed over the computers within the Spark cluster. Programs enqueue
data-parallel transformations on the Spark master, which are then interpreted into a directed-acyclic
graph~(DAG) for execution; this approach is similar to the DAG scheduling approach pioneered in
Dryad~\cite{isard07}. Spark executes the DAG whenever a disk shuffle is required, or if a newly
enqueued operation would create a cycle in the DAG~(i.e., an iterative computation is scheduled).

To execute a stage, the Spark master serializes the user defined function~(UDF) which is being applied,
transmits this function to all worker nodes, and then applies the computation. While the general
application programming interface~(API) that Spark provides is data-parallel across all elements,
the API transformations are implemented by applying the transformation iteratively across all elements
of a partition---Spark also exposes this parallel-by-partition interface via the \texttt{mapPartitions} call.
If data must be moved between partitions after an execution stage, a disk shuffle will occur. Disk
shuffles are analogous to an execution stage with interleaved computation and all-to-all communication
followed by a barrier before the next stage. This process is similar to how the results of \texttt{map}
phases are moved to \texttt{reducers} in both MapReduce and Hadoop~\cite{dean04}---the main
distinction is that Spark allows successive \texttt{map} phases, and that shuffles do not occur after all
stages.

\section{Implementation}
\label{sec:implementation}

\subsection{Characterizing and Modeling Imbalance}
\label{sec:imbalance}

\subsection{Partitioning Strategies}
\label{sec:partitioning-strategies}

\section{Performance}
\label{sec:performance}

\section{Discussion}
\label{sec:discussion}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{acm}

\bibliography{snark}

\end{document}