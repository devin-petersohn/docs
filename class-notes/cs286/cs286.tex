\documentclass[10pt]{article}
\topmargin=0.0in %length of margin at the top of the page (1 inch added by default)
\oddsidemargin=0.0in %length of margin on sides for odd pages
\evensidemargin=0in %length of margin on sides for even pages
\textwidth=6.5in %How wide you want your text to be
\marginparwidth=0.5in
\headheight=0pt %1in margins at top and bottom (1 inch is added to this value by default)
\headsep=0pt %Increase to increase white space in between headers and the top of the page
\textheight=9.1in %How tall the text body is allowed to be on each page

\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{enumitem}

\begin{document}

\date{}
\title{CS286: Database Systems}

\maketitle

\section{Lecture 3---9/4/2014}

\subsection{R*}

\begin{itemize}
\item Assumptions:
\begin{itemize}
\item There are administrative causes behind distributed data
\item Network: unreliable transport, in-order, packets are intact
\item Independent node failure
\item Slow-ish network
\end{itemize}
\item Research goals:
\begin{itemize}
\item ``Site autonomy'': No centralized state or control
\begin{itemize}
\item Data you touch should determine the sites you talk to
\item ``Distributed system is a system that fails because a machine you've never heard of fails''
\item Load sharing and decentralization
\item Less communication
\item Harder to coordinate data consistency
\item More network connections beyond hub and spoke
\item Metadata management is harder
\end{itemize}
\item Location transparency $\rightarrow$ emulate a centralized DB
\item Don't assume much about the network or OS
\end{itemize}
\item Highlights:
\begin{itemize}
\item Query optimizer cost modeling
\item Data layouts $\rightarrow$ horizontal partitioning
\item Replication
\item Distribution
\item Query compilation---unclear as to balance between compilation overhead and work saving
\item Spent a lot of time talking about 2PC $\rightarrow$ presumed commit
\end{itemize}
\end{itemize}

\subsection{Gamma}

\begin{itemize}
\item Assumptions:
\begin{itemize}
\item Fast interconnect---hypercube, more network bandwidth than aggregate disk bandwidth
\item Shared nothing---no disk or memory sharing
\end{itemize}
\item Research goals:
\begin{itemize}
\item Scale
\end{itemize}
\item Highlights:
\begin{itemize}
\item Parallel hybrid-hash join
\item Chained declustering
\end{itemize}
\item Assess:
\begin{itemize}
\item Linear speedup + scale-up
\item Superlinear speedup due to minimized seek count at scale
\end{itemize}
\end{itemize}

\section{Lecture 4---9/9/2014}

\begin{itemize}
\item ACID
\begin{itemize}
\item Consistency is not what we typically think
\item Distributed systems: data has a consistent value across sites
\item Databases: data meets contract when transaction completes
\end{itemize}
\item Serializability mathematically gives atomicity and isolation
\item Logging gives atomicity and durability
\item Ordering:
\begin{itemize}
\item Determines outcome (unless operations are not associative and commutative)
\item Some things are commutable/associable
\item Ordering must be equivalent to some serializable order
\item Implicitly, this provides an API---people don't \emph{need} to reason about concurrency
\end{itemize}
\item What is storage?
\begin{itemize}
\item \emph{Spacial-temporal rendezvous makes everything work!!!!}
\end{itemize}
\item Want to avoid/undo conflicts in space and time
\begin{itemize}
\item \emph{Space:} Shared names
\item \emph{Time:} Ordering
\end{itemize}
\item 2PL: Provides a conflict serialized schedule
\begin{itemize}
\item Ordered by race for locks
\item Ordered by the end of the first phase (``lock point'')
\end{itemize}
\item Multi-version timestamp ordering
\begin{itemize}
\item Every transaction gets a timestamp---this is the only synchronization point
\item For every object:
\begin{itemize}
\item Writes generate a new version for an object
\item Reads annotate the version for the object
\end{itemize}
\end{itemize}
\end{itemize}

\section{Lecture 5---9/11/2014}

\begin{itemize}
\item Good graphs:
\begin{itemize}
\item Crossover points
\item Non-monotonicity
\item Good breadth of X
\item Smooth $\rightarrow$ variance was accounted for
\end{itemize}
\item Infinite resources:
\begin{itemize}
\item Why run infinite resources? Many people assumed infinite resources in their papers.
\item OCC wins because it allows higher parallelism, at the cost of restarting transactions
\item Blocking (2PL) performs well at start, low at the end. Why?
\begin{itemize}
\item Deadlock starts to cause performance to fail
\item Lock contention starts to cause transactions to get in each other's way
\item Locking is a feedback loop---it lengthens transaction time
\end{itemize}
\end{itemize}
\item Takeaways:
\begin{itemize}
\item MPL is a control variable---choose your infrastructure for your system
\end{itemize}
\item When do we have ``infinite'' resources?
\begin{itemize}
\item When we have user interaction (Computer $\gg$ human)
\item Vastly overprovisioned compute
\item Work is not going on inside the serving infrastructure (e.g., work is done by clients)
\end{itemize}
\end{itemize}

\subsection{What happens when you go distributed?}

\begin{itemize}
\item Why go distributed?
\begin{itemize}
\item Capacity (storage and throughput)
\item Low latency (tolerance)
\item Fault tolerance (durability vs. availability)
\end{itemize}
\item Techniques
\begin{itemize}
\item Sharding---split dataset across many nodes
\item Replication
\end{itemize}
\end{itemize}

\section{Lecture 6---9/16/2014}

\begin{itemize}
\item You need replication $\rightarrow$ resilience to failure
\item Tradeoff between replication and performance
\item NoSQL:
\begin{itemize}
\item Typically, a key-value store (data/programming model)
\item Typically distributed and sharded/partitioned
\item Usually weaker consistency model
\item No transactions/weak isolation model
\item ``Not MySQL'' $\rightarrow$ lots of work at AOL/etc. with MySQL on \texttt{memcached}
\item Typically OSS, not enterprise
\item ``Scalable'', especially incremental scale $\rightarrow$ improves organization/administration/ops
\item ``Evaporation'' of the DBA
\end{itemize}
\item Motivations:
\begin{itemize}
\item Bayou: I want to operate when disconnected
\item Dynamo: Nodes gonna fail
\end{itemize}
\item CAP theorem: if partitions occur, then we can either have consistency or availability
\begin{itemize}
\item Availability: As long as a client can access a server, I can access data (concurrent operations
don't need to communicate)
\item Consistency: ``linearizable registers'' $\rightarrow$ if I make a write, you can read my write
\end{itemize}
\end{itemize}

\section{Lecture 7---9/18/2014}

\begin{itemize}
\item In traditional database, have disk page with tuples stored at continuous offsets.
\begin{itemize}
\item Pointers (``slots'') are at end of page and point back to tuples.
\item Can then compress and compact by looking at slot pointers.
\item Fixed length fields stored in tuples
\item Tuples contain pointers to variable length fields
\end{itemize}
\item What changed between 1980 and 2010?
\begin{itemize}
\item CPUs 10,000$\times$ faster
\item Disk BW grew 100$\times$
\item Disk seek time improved 10$\times$
\end{itemize}
\item Specifically, gulf between disk performance and processor performance grew
\item Research methodology: if area is fairly static, change parameters and see what you can do
\item MonetDB
\begin{itemize}
\item Vector/block processing:
\end{itemize}
\item Traditional iterator processing model:
\begin{itemize}
\item Build a tree of operators that run on top of iterators
\item Algorithms have init method (set up state), get next (give me a tuple), and close operators
\item ``Pull'' model $\rightarrow$ data and control flow are coupled
\end{itemize}
\item ``Late materialization:'' query optimizer should defer reading columns until as late as it can
\item ``Invisible joins:'' joins that batch reordering
\begin{itemize}
\item Semijoin: Filter R for all items that have a match in S
\end{itemize}
\item Database cracking: opportunistically reorder blocks in order to improve performance
\end{itemize}

\section{Lecture 8---9/23/2014}

\begin{itemize}
\item Pre-relational data models:
\begin{itemize}
\item Network: objects + pointers
\item Hierarchical: nested sets
\end{itemize}
\item Then, \emph{the Relational Revolution}
\item But, persistent questioning:
\begin{itemize}
\item In the 80's, nested relational
\item Object-Oriented Database $\rightarrow$ 80's/90's
\item And then... XML
\end{itemize}
\item Why flatten into relations:
\begin{itemize}
\item Space efficient
\item Update/delete/inserts require work/care
\item Simple model and language
\item Data independence: physical and logical independence
\end{itemize}
\item When is the relational model a pain?
\begin{itemize}
\item Joins are expensive
\item Must know schema ahead of time
\item Read-only workloads are expensive
\item Programming language state
\end{itemize}
\item Engineering versus ``Found Structure'' $\rightarrow$ \emph{bricolage}
\begin{itemize}
\item Engineering $\rightarrow$ collaboration and communication
\item Found structure $\rightarrow$ exploratory data analytics
\end{itemize}
\item XML database history:
\begin{itemize}
\item WWW + search ate DB lunch
\item Let's query the internets!
\item DB $\times$ WWW $\times$ markup language people = pandemonium
\item 4 data models, 3 query languages, mostly overlap\dots
\end{itemize}
\item DB vendors kept up with the pace of research
\item How to encode XML:
\begin{itemize}
\item Native
\item Shred to relationable tables
\item Relational encoding of trees
\item Path/value encoding
\item Hybrids
\end{itemize}
\end{itemize}

\section{Lecture 9---9/25/2014}

\begin{itemize}
\item Few ways to provide/describe isolation; e.g., for a KV store:
\begin{itemize}
\item 2PL $\rightarrow$ mechanism
\item Avoid anomalies (no lost update, no dirty/fuzzy read) $\rightarrow$ anomaly prevention
\item Draw conflicts into graph (graph should have no cycle) $\rightarrow$ graph formalism
\item Every history is view equivalent to a serial history $\rightarrow$ equivalence formalism
\item If every program preserves an invariant, then every execution will preserve the invariant $\rightarrow$
integrity
\end{itemize}
\item Full isolation is expensive; let's go for weaker models
\item These descriptions don't imply equivalence:
\begin{itemize}
\item Can provide SG without cycle with OCC, OCC $\ne$ 2PL
\item Snapshot isolation $\rightarrow$ no anomalies, but does not preserve invariants
\end{itemize}
\item Strong vs. weak isolation: weaker implies more anomalies allowed, or more executions allowed
\item Conditions of \emph{reality}:
\begin{itemize}
\item Phantom: occurs when your program has a complex predicate, and when table modifications are
allowed (modifications can change predicate selections)
\begin{itemize}
\item Can solve with predicate locks (but no one does that)
\item Can solve with locks on indices, next key locks, etc\dots
\end{itemize}
\item \emph{Repeatable read}: in SQL, an isolation level where you can have phantoms, but everything
else is OK (IBM defines as full isolation, hence confusion\dots)
\item \emph{Fuzzy read}: short read locks, I read a single item multiple times and can see different
values
\item Statement-level atomicity: hold short read locks for the whole time I'm evaluating a statement
\item \texttt{select \dots \ for update}: hint that I'll grab locks later
\end{itemize}
\item Isolation levels:
\begin{enumerate}[start = 0]
\item Short duration write locks
\item Commit duration write locks
\item Commit duration write locks, short read locks
\item Full serializability
\end{enumerate}
\item Snapshot isolation: a good demonstration that you can beat anomalies (no lost update, no dirty
read, no fuzzy read, no phantoms), but not provide serializability
\end{itemize}

\end{document}