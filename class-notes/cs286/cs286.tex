\documentclass[10pt]{article}
\topmargin=0.0in %length of margin at the top of the page (1 inch added by default)
\oddsidemargin=0.0in %length of margin on sides for odd pages
\evensidemargin=0in %length of margin on sides for even pages
\textwidth=6.5in %How wide you want your text to be
\marginparwidth=0.5in
\headheight=0pt %1in margins at top and bottom (1 inch is added to this value by default)
\headsep=0pt %Increase to increase white space in between headers and the top of the page
\textheight=9.1in %How tall the text body is allowed to be on each page

\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}

\begin{document}

\date{}
\title{CS286: Database Systems}

\maketitle

\section{Lecture 3---9/4/2014}

\subsection{R*}

\begin{itemize}
\item Assumptions:
\begin{itemize}
\item There are administrative causes behind distributed data
\item Network: unreliable transport, in-order, packets are intact
\item Independent node failure
\item Slow-ish network
\end{itemize}
\item Research goals:
\begin{itemize}
\item ``Site autonomy'': No centralized state or control
\begin{itemize}
\item Data you touch should determine the sites you talk to
\item ``Distributed system is a system that fails because a machine you've never heard of fails''
\item Load sharing and decentralization
\item Less communication
\item Harder to coordinate data consistency
\item More network connections beyond hub and spoke
\item Metadata management is harder
\end{itemize}
\item Location transparency $\rightarrow$ emulate a centralized DB
\item Don't assume much about the network or OS
\end{itemize}
\item Highlights:
\begin{itemize}
\item Query optimizer cost modeling
\item Data layouts $\rightarrow$ horizontal partitioning
\item Replication
\item Distribution
\item Query compilation---unclear as to balance between compilation overhead and work saving
\item Spent a lot of time talking about 2PC $\rightarrow$ presumed commit
\end{itemize}
\end{itemize}

\subsection{Gamma}

\begin{itemize}
\item Assumptions:
\begin{itemize}
\item Fast interconnect---hypercube, more network bandwidth than aggregate disk bandwidth
\item Shared nothing---no disk or memory sharing
\end{itemize}
\item Research goals:
\begin{itemize}
\item Scale
\end{itemize}
\item Highlights:
\begin{itemize}
\item Parallel hybrid-hash join
\item Chained declustering
\end{itemize}
\item Assess:
\begin{itemize}
\item Linear speedup + scale-up
\item Superlinear speedup due to minimized seek count at scale
\end{itemize}
\end{itemize}

\section{Lecture 4---9/9/2014}

\begin{itemize}
\item ACID
\begin{itemize}
\item Consistency is not what we typically think
\item Distributed systems: data has a consistent value across sites
\item Databases: data meets contract when transaction completes
\end{itemize}
\item Serializability mathematically gives atomicity and isolation
\item Logging gives atomicity and durability
\item Ordering:
\begin{itemize}
\item Determines outcome (unless operations are not associative and commutative)
\item Some things are commutable/associable
\item Ordering must be equivalent to some serializable order
\item Implicitly, this provides an API---people don't \emph{need} to reason about concurrency
\end{itemize}
\item What is storage?
\begin{itemize}
\item \emph{Spacial-temporal rendezvous makes everything work!!!!}
\end{itemize}
\item Want to avoid/undo conflicts in space and time
\begin{itemize}
\item \emph{Space:} Shared names
\item \emph{Time:} Ordering
\end{itemize}
\item 2PL: Provides a conflict serialized schedule
\begin{itemize}
\item Ordered by race for locks
\item Ordered by the end of the first phase (``lock point'')
\end{itemize}
\item Multi-version timestamp ordering
\begin{itemize}
\item Every transaction gets a timestamp---this is the only synchronization point
\item For every object:
\begin{itemize}
\item Writes generate a new version for an object
\item Reads annotate the version for the object
\end{itemize}
\end{itemize}
\end{itemize}

\section{Lecture 5---9/11/2014}

\begin{itemize}
\item Good graphs:
\begin{itemize}
\item Crossover points
\item Non-monotonicity
\item Good breadth of X
\item Smooth $\rightarrow$ variance was accounted for
\end{itemize}
\item Infinite resources:
\begin{itemize}
\item Why run infinite resources? Many people assumed infinite resources in their papers.
\item OCC wins because it allows higher parallelism, at the cost of restarting transactions
\item Blocking (2PL) performs well at start, low at the end. Why?
\begin{itemize}
\item Deadlock starts to cause performance to fail
\item Lock contention starts to cause transactions to get in each other's way
\item Locking is a feedback loop---it lengthens transaction time
\end{itemize}
\end{itemize}
\item Takeaways:
\begin{itemize}
\item MPL is a control variable---choose your infrastructure for your system
\end{itemize}
\item When do we have ``infinite'' resources?
\begin{itemize}
\item When we have user interaction (Computer $\gg$ human)
\item Vastly overprovisioned compute
\item Work is not going on inside the serving infrastructure (e.g., work is done by clients)
\end{itemize}
\end{itemize}

\subsection{What happens when you go distributed?}

\begin{itemize}
\item Why go distributed?
\begin{itemize}
\item Capacity (storage and throughput)
\item Low latency (tolerance)
\item Fault tolerance (durability vs. availability)
\end{itemize}
\item Techniques
\begin{itemize}
\item Sharding---split dataset across many nodes
\item Replication
\end{itemize}
\end{itemize}

\end{document}